{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d327cdad",
   "metadata": {},
   "source": [
    "# Café Sales Data Analysis - Data Cleaning & Preparation\n",
    "\n",
    "## Executive Summary\n",
    "This notebook provides comprehensive data cleaning and preparation for café sales analysis. We process 10,000 transactions from 2023 to ensure data quality and reliability for subsequent business intelligence analysis. The cleaning process addresses missing values, data type inconsistencies, and implements strategic imputation methods based on statistical analysis.\n",
    "\n",
    "\n",
    "## Data Quality Objectives\n",
    "1. **Completeness**: Address missing values using statistically sound imputation methods\n",
    "2. **Consistency**: Standardize data types and formats across all columns\n",
    "3. **Accuracy**: Validate business logic and recalculate derived fields\n",
    "4. **Reliability**: Remove or flag problematic records that could skew analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74cfeff",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Import\n",
    "\n",
    "### 1.1 Library Imports\n",
    "We import essential libraries for data manipulation and analysis. These form the foundation of our data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29316201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data cleaning and analysis\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np   # Numerical computing and array operations\n",
    "import matplotlib.pyplot as plt  # Data visualization (for validation plots if needed)\n",
    "\n",
    "# Configure pandas display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_import_section",
   "metadata": {},
   "source": [
    "### 1.2 Data Import and Initial Exploration\n",
    "\n",
    "**Strategy**: We use strategic NA value handling during import to catch common data quality issues:\n",
    "- \"UNKNOWN\" values indicate missing product information\n",
    "- \"ERROR\" values suggest system failures during data collection\n",
    "- Empty strings represent incomplete form submissions\n",
    "\n",
    "This approach allows us to identify and quantify data quality issues from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c3d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: 10,000 rows × 8 columns\n",
      "Total transactions: 10,000\n",
      "Date range coverage: 365 unique dates\n"
     ]
    }
   ],
   "source": [
    "# Load the raw café sales data with comprehensive NA value detection\n",
    "# Note: Adjust the file path as needed for your environment\n",
    "df = pd.read_csv('/Users/kabbo/Desktop/marcy/Project 2 Cafe Sales/M1-FINAL-PROJECT-THIERNO_KABBO/data/raw/cafe_sales.csv', \n",
    "                 na_values=[\"UNKNOWN\", \"ERROR\", \"\"])\n",
    "\n",
    "# Initial data exploration to understand the dataset structure\n",
    "print(f\"Dataset dimensions: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Total transactions: {df.shape[0]:,}\")\n",
    "print(f\"Date range coverage: {df['Transaction Date'].nunique()} unique dates\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_quality_assessment",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "### 2.1 Data Types and Structure Analysis\n",
    "\n",
    "Understanding the current data types helps us identify necessary conversions and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0851f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Transaction ID    10000 non-null  object \n",
      " 1   Item              9031 non-null   object \n",
      " 2   Quantity          9521 non-null   float64\n",
      " 3   Price Per Unit    9467 non-null   float64\n",
      " 4   Total Spent       9498 non-null   float64\n",
      " 5   Payment Method    6822 non-null   object \n",
      " 6   Location          6039 non-null   object \n",
      " 7   Transaction Date  9540 non-null   object \n",
      "dtypes: float64(3), object(5)\n",
      "memory usage: 625.1+ KB\n",
      "  • Transaction ID: object\n",
      "  • Item: object\n",
      "  • Quantity: float64\n",
      "  • Price Per Unit: float64\n",
      "  • Total Spent: float64\n",
      "  • Payment Method: object\n",
      "  • Location: object\n",
      "  • Transaction Date: object\n",
      "  • Transaction ID: 10,000 unique values\n",
      "  • Item: 8 unique values\n",
      "  • Quantity: 5 unique values\n",
      "  • Price Per Unit: 6 unique values\n",
      "  • Total Spent: 17 unique values\n",
      "  • Payment Method: 3 unique values\n",
      "  • Location: 2 unique values\n",
      "  • Transaction Date: 365 unique values\n"
     ]
    }
   ],
   "source": [
    "#  Data structure analysis\n",
    "df.info()\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"  • {col}: {df[col].dtype}\")\n",
    "\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"  • {col}: {unique_count:,} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing_values_analysis",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis\n",
    "\n",
    "**Business Impact**: Missing values can significantly impact analysis accuracy. We analyze patterns to understand:\n",
    "- Whether missing values are random or systematic\n",
    "- Which columns are most affected\n",
    "- Potential business reasons for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93d1b374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Missing Count  Missing Percentage\n",
      "Location                   3961               39.61\n",
      "Payment Method             3178               31.78\n",
      "Item                        969                9.69\n",
      "Price Per Unit              533                5.33\n",
      "Total Spent                 502                5.02\n",
      "Quantity                    479                4.79\n",
      "Transaction Date            460                4.60\n",
      "Transaction ID                0                0.00\n"
     ]
    }
   ],
   "source": [
    "# Detailed missing values analysis \n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent.round(2)\n",
    "})\n",
    "\n",
    "# Sort by missing percentage to prioritize cleaning efforts\n",
    "missing_summary = missing_summary.sort_values('Missing Percentage', ascending=False)\n",
    "print(missing_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate_analysis",
   "metadata": {},
   "source": [
    "### 2.3 Duplicate Records Analysis\n",
    "\n",
    "**Methodology**: We check for duplicates across key business fields to identify potential data entry errors or system issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fae47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "Empty DataFrame\n",
      "Columns: [Transaction ID, Item, Quantity, Price Per Unit, Total Spent, Payment Method, Location, Transaction Date]\n",
      "Index: []\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "duplicate_rows = df[df.duplicated(subset=['Transaction ID', 'Item', 'Quantity', 'Price Per Unit', 'Transaction Date'], keep=False)]\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "num_duplicates = df.duplicated(subset=['Transaction ID', 'Item', 'Quantity', 'Price Per Unit', 'Transaction Date'], keep=False).sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning_strategy",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning Implementation\n",
    "\n",
    "### 3.1 Item Column Cleaning Strategy\n",
    "\n",
    "**Business Decision**: We preserve \"UNKNOWN\" items rather than imputing them because:\n",
    "1. They represent real business scenarios (unscanned items, manual entries)\n",
    "2. Imputation could mask operational issues that need addressing\n",
    "3. They can be analyzed separately to understand system gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b0851f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item distribution after cleaning:\n",
      "Item\n",
      "Juice       1171\n",
      "Coffee      1165\n",
      "Salad       1148\n",
      "Cake        1139\n",
      "Sandwich    1131\n",
      "Smoothie    1096\n",
      "Cookie      1092\n",
      "Tea         1089\n",
      "UNKNOWN      969\n",
      "Name: count, dtype: int64\n",
      "Unknown Percentage:  9.69\n"
     ]
    }
   ],
   "source": [
    "# Strategic handling of missing Item values\n",
    "\n",
    "# Convert NaN values to 'UNKNOWN' for explicit tracking\n",
    "df['Item'] = df['Item'].fillna('UNKNOWN')\n",
    "\n",
    "# Analyze item distribution after cleaning\n",
    "item_distribution = df['Item'].value_counts()\n",
    "print(\"Item distribution after cleaning:\")\n",
    "print(item_distribution)\n",
    "\n",
    "# Calculate percentage of unknown items\n",
    "unknown_percentage = (item_distribution['UNKNOWN'] / len(df)) * 100\n",
    "print(\"Unknown Percentage: \", unknown_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantity_imputation",
   "metadata": {},
   "source": [
    "### 3.2 Quantity Imputation Strategy\n",
    "\n",
    "**Statistical Approach**: Based on preliminary analysis (referenced from Excel analysis), the Quantity distribution shows slight skewness. We use median imputation because:\n",
    "1. **Robust to outliers**: Median is less affected by extreme values\n",
    "2. **Business logic**: Most café transactions involve 1-3 items\n",
    "3. **Conservative approach**: Avoids inflating average transaction sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52e7fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity statistics before imputation:\n",
      "count    10000.000000\n",
      "mean         3.027100\n",
      "std          1.384614\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          4.000000\n",
      "max          5.000000\n",
      "Name: Quantity, dtype: float64\n",
      "\n",
      "Imputation details:\n",
      "• Missing values: 0 (0.0%)\n",
      "• Median value used for imputation: 3.0\n",
      "• Rationale: Median chosen due to skewed distribution (robust to outliers)\n"
     ]
    }
   ],
   "source": [
    "# Analyze quantity distribution before imputation\n",
    "print(\"Quantity statistics before imputation:\")\n",
    "print(df['Quantity'].describe())\n",
    "\n",
    "# Calculate imputation value\n",
    "quantity_median = df['Quantity'].median()\n",
    "missing_quantity_count = df['Quantity'].isnull().sum()\n",
    "\n",
    "print(f\"\\nImputation details:\")\n",
    "print(f\"• Missing values: {missing_quantity_count:,} ({missing_quantity_count/len(df)*100:.1f}%)\")\n",
    "print(f\"• Median value used for imputation: {quantity_median}\")\n",
    "print(f\"• Rationale: Median chosen due to skewed distribution (robust to outliers)\")\n",
    "\n",
    "# Perform imputation\n",
    "df['Quantity'] = df['Quantity'].fillna(quantity_median)\n",
    "\n",
    "# Convert to integer for business logic (can't have fractional items)\n",
    "df['Quantity'] = df['Quantity'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "price_imputation",
   "metadata": {},
   "source": [
    "### 3.3 Price Per Unit Imputation Strategy\n",
    "\n",
    "**Statistical Approach**: Price Per Unit shows normal distribution characteristics, making mean imputation appropriate:\n",
    "1. **Central tendency**: Mean represents typical pricing\n",
    "2. **Business continuity**: Maintains overall revenue patterns\n",
    "3. **Minimal bias**: Normal distribution supports mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c69d97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Per Unit statistics before imputation:\n",
      "count    10000.000000\n",
      "mean         2.949984\n",
      "std          1.243910\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          4.000000\n",
      "max          5.000000\n",
      "Name: Price Per Unit, dtype: float64\n",
      "\n",
      "Imputation details:\n",
      "• Missing values: 0 (0.0%)\n",
      "• Mean value used for imputation: $2.95\n",
      "• Rationale: Mean chosen due to normal distribution of prices\n"
     ]
    }
   ],
   "source": [
    "# Analyze price distribution before imputation\n",
    "print(\"Price Per Unit statistics before imputation:\")\n",
    "print(df['Price Per Unit'].describe())\n",
    "\n",
    "# Calculate imputation value\n",
    "price_per_unit_mean = df['Price Per Unit'].mean()\n",
    "missing_price_count = df['Price Per Unit'].isnull().sum()\n",
    "\n",
    "print(f\"\\nImputation details:\")\n",
    "print(f\"• Missing values: {missing_price_count:,} ({missing_price_count/len(df)*100:.1f}%)\")\n",
    "print(f\"• Mean value used for imputation: ${price_per_unit_mean:.2f}\")\n",
    "print(f\"• Rationale: Mean chosen due to normal distribution of prices\")\n",
    "\n",
    "# Perform imputation\n",
    "df['Price Per Unit'] = df['Price Per Unit'].fillna(price_per_unit_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "total_spent_calculation",
   "metadata": {},
   "source": [
    "### 3.4 Total Spent Recalculation\n",
    "\n",
    "**Business Logic**: Rather than imputing Total Spent, we recalculate it to ensure data consistency:\n",
    "1. **Data integrity**: Eliminates discrepancies between components and totals\n",
    "2. **Audit trail**: Clear calculation method for financial validation\n",
    "3. **Business accuracy**: Ensures all financial metrics are mathematically correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Item  Quantity  Price Per Unit  Total Spent\n",
      "0    Coffee         2             2.0          4.0\n",
      "1      Cake         4             3.0         12.0\n",
      "2    Cookie         4             1.0          4.0\n",
      "3     Salad         2             5.0         10.0\n",
      "4    Coffee         2             2.0          4.0\n",
      "5  Smoothie         5             4.0         20.0\n",
      "6   UNKNOWN         3             3.0          9.0\n",
      "7  Sandwich         4             4.0         16.0\n",
      "8   UNKNOWN         5             3.0         15.0\n",
      "9  Sandwich         5             4.0         20.0\n"
     ]
    }
   ],
   "source": [
    "# Recalculate Total Spent\n",
    "df['Total Spent'] = df['Price Per Unit'] * df['Quantity']\n",
    "\n",
    "\n",
    "# Checking and comparing with Excel results \n",
    "print(df[['Item', 'Quantity', 'Price Per Unit', 'Total Spent']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorical_data_strategy",
   "metadata": {},
   "source": [
    "### 3.5 Categorical Data Handling Strategy\n",
    "\n",
    "**Business Decision**: For Payment Method and Location, we preserve missing values because:\n",
    "1. **Operational insights**: Missing data patterns may indicate system issues\n",
    "2. **Customer behavior**: Some customers may prefer not to specify location\n",
    "3. **Data integrity**: Imputation could mask important business problems\n",
    "\n",
    "These fields will be analyzed separately to understand missing data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "date_cleaning",
   "metadata": {},
   "source": [
    "### 3.6 Transaction Date Cleaning\n",
    "\n",
    "**Critical Business Decision**: Transaction dates are essential for time-series analysis. Missing dates cannot be reliably imputed without introducing bias, so we remove these records:\n",
    "1. **Temporal analysis integrity**: Date is crucial for trend analysis\n",
    "2. **Business reporting**: All financial reports require valid dates\n",
    "3. **Data quality**: Better to have complete records than imputed dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5236c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9540\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['Transaction Date'])\n",
    "\n",
    "print(df['Transaction Date'].isnull().sum())  # Should print 0\n",
    "print(df['Transaction Date'].count())        # See all unique values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting cleaned data for next steps\n",
    "\n",
    "df.to_csv('cleaned_cafe_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba67eca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
